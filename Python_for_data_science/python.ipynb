{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc28d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Sam\", \"age\": 34}\n",
      "{'name': 'Sam', 'age': 34}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Data serialization\n",
    "import json  \n",
    "data = {'name': 'Sam', 'age': 34}\n",
    "json_str = json.dumps(data)\n",
    "print(json_str)\n",
    "\n",
    "parsed_data = json.loads(json_str)\n",
    "print(parsed_data)\n",
    "print(type(parsed_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91b6997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'age']\n",
      "['Sam', '43']\n"
     ]
    }
   ],
   "source": [
    "# csv\n",
    "import csv\n",
    "with open('example.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['name', 'age'])\n",
    "    writer.writerow(['Sam', 43])\n",
    "\n",
    "# read\n",
    "with open('example.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78af0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:35:28.727505\n",
      "2025-05-13 14:35:28.727505\n"
     ]
    }
   ],
   "source": [
    "# datetime\n",
    "from datetime import datetime, timedelta\n",
    "now = datetime.now()\n",
    "print(now)\n",
    "\n",
    "yesterday = now - timedelta(days=1)\n",
    "print(yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3104b38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747222597.405282\n",
      "1747222599.40748\n"
     ]
    }
   ],
   "source": [
    "# time\n",
    "import time\n",
    "\n",
    "print(time.time())\n",
    "time.sleep(2)\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80344acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "# regular expressions\n",
    "import re\n",
    "\n",
    "pattern = r'\\d+' # d -> digits\n",
    "text = 'There are 123 apples'\n",
    "match = re.search(pattern, text)\n",
    "print(match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe80bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"sepal.length\",\"sepal.width\",\"petal.length\",\"petal.width\",\"variety\"\n",
      "5.1,3.5,1.4,.2,\"Setosa\"\n",
      "4.9,3,1.4,.2,\"Setosa\"\n",
      "4.7,3.2,1.3,.2,\"Setosa\"\n",
      "4.6,3.1,1.5,.2,\"Setosa\"\n",
      "5,3.6,1.4,.2,\"Setosa\"\n",
      "5.4,3.9,1.7,.4,\"Setosa\"\n",
      "4.6,3.4,1.4,.3,\"Setosa\"\n",
      "5,3.4,1.5,.2,\"Setosa\"\n",
      "4.4,2.9,1.4,.2,\"Setosa\"\n",
      "4.9,3.1,1.5,.1,\"Setosa\"\n",
      "5.4,3.7,1.5,.2,\"Setosa\"\n",
      "4.8,3.4,1.6,.2,\"Setosa\"\n",
      "4.8,3,1.4,.1,\"Setosa\"\n",
      "4.3,3,1.1,.1,\"Setosa\"\n",
      "5.8,4,1.2,.2,\"Setosa\"\n",
      "5.7,4.4,1.5,.4,\"Setosa\"\n",
      "5.4,3.9,1.3,.4,\"Setosa\"\n",
      "5.1,3.5,1.4,.3,\"Setosa\"\n",
      "5.7,3.8,1.7,.3,\"Setosa\"\n",
      "5.1,3.8,1.5,.3,\"Setosa\"\n",
      "5.4,3.4,1.7,.2,\"Setosa\"\n",
      "5.1,3.7,1.5,.4,\"Setosa\"\n",
      "4.6,3.6,1,.2,\"Setosa\"\n",
      "5.1,3.3,1.7,.5,\"Setosa\"\n",
      "4.8,3.4,1.9,.2,\"Setosa\"\n",
      "5,3,1.6,.2,\"Setosa\"\n",
      "5,3.4,1.6,.4,\"Setosa\"\n",
      "5.2,3.5,1.5,.2,\"Setosa\"\n",
      "5.2,3.4,1.4,.2,\"Setosa\"\n",
      "4.7,3.2,1.6,.2,\"Setosa\"\n",
      "4.8,3.1,1.6,.2,\"Setosa\"\n",
      "5.4,3.4,1.5,.4,\"Setosa\"\n",
      "5.2,4.1,1.5,.1,\"Setosa\"\n",
      "5.5,4.2,1.4,.2,\"Setosa\"\n",
      "4.9,3.1,1.5,.2,\"Setosa\"\n",
      "5,3.2,1.2,.2,\"Setosa\"\n",
      "5.5,3.5,1.3,.2,\"Setosa\"\n",
      "4.9,3.6,1.4,.1,\"Setosa\"\n",
      "4.4,3,1.3,.2,\"Setosa\"\n",
      "5.1,3.4,1.5,.2,\"Setosa\"\n",
      "5,3.5,1.3,.3,\"Setosa\"\n",
      "4.5,2.3,1.3,.3,\"Setosa\"\n",
      "4.4,3.2,1.3,.2,\"Setosa\"\n",
      "5,3.5,1.6,.6,\"Setosa\"\n",
      "5.1,3.8,1.9,.4,\"Setosa\"\n",
      "4.8,3,1.4,.3,\"Setosa\"\n",
      "5.1,3.8,1.6,.2,\"Setosa\"\n",
      "4.6,3.2,1.4,.2,\"Setosa\"\n",
      "5.3,3.7,1.5,.2,\"Setosa\"\n",
      "5,3.3,1.4,.2,\"Setosa\"\n",
      "7,3.2,4.7,1.4,\"Versicolor\"\n",
      "6.4,3.2,4.5,1.5,\"Versicolor\"\n",
      "6.9,3.1,4.9,1.5,\"Versicolor\"\n",
      "5.5,2.3,4,1.3,\"Versicolor\"\n",
      "6.5,2.8,4.6,1.5,\"Versicolor\"\n",
      "5.7,2.8,4.5,1.3,\"Versicolor\"\n",
      "6.3,3.3,4.7,1.6,\"Versicolor\"\n",
      "4.9,2.4,3.3,1,\"Versicolor\"\n",
      "6.6,2.9,4.6,1.3,\"Versicolor\"\n",
      "5.2,2.7,3.9,1.4,\"Versicolor\"\n",
      "5,2,3.5,1,\"Versicolor\"\n",
      "5.9,3,4.2,1.5,\"Versicolor\"\n",
      "6,2.2,4,1,\"Versicolor\"\n",
      "6.1,2.9,4.7,1.4,\"Versicolor\"\n",
      "5.6,2.9,3.6,1.3,\"Versicolor\"\n",
      "6.7,3.1,4.4,1.4,\"Versicolor\"\n",
      "5.6,3,4.5,1.5,\"Versicolor\"\n",
      "5.8,2.7,4.1,1,\"Versicolor\"\n",
      "6.2,2.2,4.5,1.5,\"Versicolor\"\n",
      "5.6,2.5,3.9,1.1,\"Versicolor\"\n",
      "5.9,3.2,4.8,1.8,\"Versicolor\"\n",
      "6.1,2.8,4,1.3,\"Versicolor\"\n",
      "6.3,2.5,4.9,1.5,\"Versicolor\"\n",
      "6.1,2.8,4.7,1.2,\"Versicolor\"\n",
      "6.4,2.9,4.3,1.3,\"Versicolor\"\n",
      "6.6,3,4.4,1.4,\"Versicolor\"\n",
      "6.8,2.8,4.8,1.4,\"Versicolor\"\n",
      "6.7,3,5,1.7,\"Versicolor\"\n",
      "6,2.9,4.5,1.5,\"Versicolor\"\n",
      "5.7,2.6,3.5,1,\"Versicolor\"\n",
      "5.5,2.4,3.8,1.1,\"Versicolor\"\n",
      "5.5,2.4,3.7,1,\"Versicolor\"\n",
      "5.8,2.7,3.9,1.2,\"Versicolor\"\n",
      "6,2.7,5.1,1.6,\"Versicolor\"\n",
      "5.4,3,4.5,1.5,\"Versicolor\"\n",
      "6,3.4,4.5,1.6,\"Versicolor\"\n",
      "6.7,3.1,4.7,1.5,\"Versicolor\"\n",
      "6.3,2.3,4.4,1.3,\"Versicolor\"\n",
      "5.6,3,4.1,1.3,\"Versicolor\"\n",
      "5.5,2.5,4,1.3,\"Versicolor\"\n",
      "5.5,2.6,4.4,1.2,\"Versicolor\"\n",
      "6.1,3,4.6,1.4,\"Versicolor\"\n",
      "5.8,2.6,4,1.2,\"Versicolor\"\n",
      "5,2.3,3.3,1,\"Versicolor\"\n",
      "5.6,2.7,4.2,1.3,\"Versicolor\"\n",
      "5.7,3,4.2,1.2,\"Versicolor\"\n",
      "5.7,2.9,4.2,1.3,\"Versicolor\"\n",
      "6.2,2.9,4.3,1.3,\"Versicolor\"\n",
      "5.1,2.5,3,1.1,\"Versicolor\"\n",
      "5.7,2.8,4.1,1.3,\"Versicolor\"\n",
      "6.3,3.3,6,2.5,\"Virginica\"\n",
      "5.8,2.7,5.1,1.9,\"Virginica\"\n",
      "7.1,3,5.9,2.1,\"Virginica\"\n",
      "6.3,2.9,5.6,1.8,\"Virginica\"\n",
      "6.5,3,5.8,2.2,\"Virginica\"\n",
      "7.6,3,6.6,2.1,\"Virginica\"\n",
      "4.9,2.5,4.5,1.7,\"Virginica\"\n",
      "7.3,2.9,6.3,1.8,\"Virginica\"\n",
      "6.7,2.5,5.8,1.8,\"Virginica\"\n",
      "7.2,3.6,6.1,2.5,\"Virginica\"\n",
      "6.5,3.2,5.1,2,\"Virginica\"\n",
      "6.4,2.7,5.3,1.9,\"Virginica\"\n",
      "6.8,3,5.5,2.1,\"Virginica\"\n",
      "5.7,2.5,5,2,\"Virginica\"\n",
      "5.8,2.8,5.1,2.4,\"Virginica\"\n",
      "6.4,3.2,5.3,2.3,\"Virginica\"\n",
      "6.5,3,5.5,1.8,\"Virginica\"\n",
      "7.7,3.8,6.7,2.2,\"Virginica\"\n",
      "7.7,2.6,6.9,2.3,\"Virginica\"\n",
      "6,2.2,5,1.5,\"Virginica\"\n",
      "6.9,3.2,5.7,2.3,\"Virginica\"\n",
      "5.6,2.8,4.9,2,\"Virginica\"\n",
      "7.7,2.8,6.7,2,\"Virginica\"\n",
      "6.3,2.7,4.9,1.8,\"Virginica\"\n",
      "6.7,3.3,5.7,2.1,\"Virginica\"\n",
      "7.2,3.2,6,1.8,\"Virginica\"\n",
      "6.2,2.8,4.8,1.8,\"Virginica\"\n",
      "6.1,3,4.9,1.8,\"Virginica\"\n",
      "6.4,2.8,5.6,2.1,\"Virginica\"\n",
      "7.2,3,5.8,1.6,\"Virginica\"\n",
      "7.4,2.8,6.1,1.9,\"Virginica\"\n",
      "7.9,3.8,6.4,2,\"Virginica\"\n",
      "6.4,2.8,5.6,2.2,\"Virginica\"\n",
      "6.3,2.8,5.1,1.5,\"Virginica\"\n",
      "6.1,2.6,5.6,1.4,\"Virginica\"\n",
      "7.7,3,6.1,2.3,\"Virginica\"\n",
      "6.3,3.4,5.6,2.4,\"Virginica\"\n",
      "6.4,3.1,5.5,1.8,\"Virginica\"\n",
      "6,3,4.8,1.8,\"Virginica\"\n",
      "6.9,3.1,5.4,2.1,\"Virginica\"\n",
      "6.7,3.1,5.6,2.4,\"Virginica\"\n",
      "6.9,3.1,5.1,2.3,\"Virginica\"\n",
      "5.8,2.7,5.1,1.9,\"Virginica\"\n",
      "6.8,3.2,5.9,2.3,\"Virginica\"\n",
      "6.7,3.3,5.7,2.5,\"Virginica\"\n",
      "6.7,3,5.2,2.3,\"Virginica\"\n",
      "6.3,2.5,5,1.9,\"Virginica\"\n",
      "6.5,3,5.2,2,\"Virginica\"\n",
      "6.2,3.4,5.4,2.3,\"Virginica\"\n",
      "5.9,3,5.1,1.8,\"Virginica\"\n"
     ]
    }
   ],
   "source": [
    "# file operations\n",
    "# Read a file\n",
    "with open('iris.csv', mode='r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c0cf926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,age\n",
      "Sam,43\n"
     ]
    }
   ],
   "source": [
    "# read line by line\n",
    "with open('example.csv', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f833e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a file\n",
    "with open('example.txt', 'a') as file: # a -> append, w -> write, w+ -> read and write\n",
    "    file.write('This is my new file')\n",
    "    file.write('Hello world')\n",
    "\n",
    "# writing a list of lines to a file\n",
    "lines=['First line\\n', 'Second line\\n', 'Third line \\n']\n",
    "with open('example.txt', 'a') as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abe702d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary files\n",
    "# writing to a binary file\n",
    "data = b'\\x00\\x01\\x02\\x03\\x04'\n",
    "with open('example.bin', 'wb') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "924ef713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x00\\x01\\x02\\x03\\x04'\n"
     ]
    }
   ],
   "source": [
    "# read binary file\n",
    "with open('example.bin', 'rb') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b75965ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.txt', 'r') as source_file:\n",
    "    content = source_file.read()\n",
    "\n",
    "with open('destination.txt', 'w') as destination_file:\n",
    "    destination_file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05f82286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines: 3, words: 21, characters: 125\n"
     ]
    }
   ],
   "source": [
    "# read a text fiel and count the nuber of lines, words and characters\n",
    "def count_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        line_count = len(lines)\n",
    "        word_count = sum(len(line.split()) for line in lines)\n",
    "        char_count = sum(len(line) for line in lines)\n",
    "    return line_count, word_count, char_count\n",
    "\n",
    "file_path = 'example.txt'\n",
    "lines, words, characters = count_text_file(file_path)\n",
    "print(f'lines: {lines}, words: {words}, characters: {characters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b85678d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love Singapore\n",
      "Also Illinois\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# writing then reading the fiel\n",
    "with open('example2.txt', 'w+') as file:\n",
    "    file.write('I love Singapore\\n')\n",
    "    file.write('Also Illinois\\n')\n",
    "\n",
    "    # move the file cursor to the beginning\n",
    "    file.seek(0)\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "754598ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory package is created\n"
     ]
    }
   ],
   "source": [
    "# create new directory\n",
    "import os\n",
    "new_directory = \"package\"\n",
    "os.mkdir(new_directory)\n",
    "print(f\"Directory {new_directory} is created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50986328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python.ipynb', 'destination.txt', 'example.bin', 'example.txt', 'Recommeder_system.ipynb', 'Data', 'package', 'ETH_1h.csv', 'example2.txt', 'iris.csv', 'notebook.ipynb', 'logistic_regression.ipynb', 'classification.ipynb', 'clustering.ipynb', 'decision_tree.ipynb', 'example.csv', 'titanic.csv']\n"
     ]
    }
   ],
   "source": [
    "# listing Files and directories\n",
    "items = os.listdir('.')\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "443f7396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wanyua/Dev/ML/Python-Machine-Learning-Tutorial/Python_for_data_science/new_folder/file.txt\n"
     ]
    }
   ],
   "source": [
    "# Joining paths\n",
    "dir_name = \"new_folder\"\n",
    "file_name = \"file.txt\"\n",
    "full_path = os.path.join(os.getcwd(),dir_name, file_name)\n",
    "print(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "104d3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path example3.txt does not exist\n"
     ]
    }
   ],
   "source": [
    "path = 'example3.txt'\n",
    "if os.path.exists(path):\n",
    "    print(f\"The path {path} exists\")\n",
    "else:\n",
    "    print(f\"The path {path} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8a71236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path example.txt is a file\n"
     ]
    }
   ],
   "source": [
    "# Checking if a path is file or directory\n",
    "import os\n",
    "path = 'example.txt'\n",
    "if os.path.isfile(path):\n",
    "    print(f\"The path {path} is a file\")\n",
    "elif os.path.isdir(path):\n",
    "    print(f\"The path {path} is a directory\")\n",
    "else:\n",
    "    print(f\"The path {path} is neither a file nor a directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "317c06ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wanyua/Dev/ML/Python-Machine-Learning-Tutorial/Python_for_data_science/example.bin\n"
     ]
    }
   ],
   "source": [
    "# getting the absolute path\n",
    "relative_path = 'example.bin'\n",
    "absolute_path = os.path.abspath(relative_path)\n",
    "print(absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8ec8fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\u0001\u0002\u0003\u0004\n",
      "File closed\n"
     ]
    }
   ],
   "source": [
    "# exceptions handling\n",
    "try:\n",
    "    file= open('example.bin', 'r')\n",
    "    content = file.read()\n",
    "    print(content)\n",
    "except FileNotFoundError:\n",
    "    print(\"The file does not exist\")\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "finally:\n",
    "    if 'file' in locals() and not file.closed:\n",
    "        file.close()\n",
    "        print('File closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ff56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.24.4\n",
      "[[1 2 3]\n",
      " [3 4 5]\n",
      " [5 4 6]]\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "import numpy as np\n",
    "arr = np.array([[1,2,3], [3,4,5], [5,4,6]])\n",
    "print(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8d1f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5dd497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511cb136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0:2, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10151930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6]\n",
      "Mean: 4.5, Standard deviation : 2.29128784747792\n",
      "Normalized data: [-1.52752523 -1.09108945 -0.65465367 -0.21821789  0.21821789  0.65465367\n",
      "  1.09108945  1.52752523]\n"
     ]
    }
   ],
   "source": [
    "# standardization and normalization\n",
    "data = np.array([1,2,3,4,5,6,7,8])\n",
    "print(data[(data > 4) & (data<7)])\n",
    "# mean and standard deviation \n",
    "# standard deviation -> tells you how much the values vary from the average\n",
    "mean = np.mean(data)\n",
    "standard_deviation = np.std(data) # standardization -> mean = 0, standardard deviation = 1\n",
    "print(f\"Mean: {mean}, Standard deviation : {standard_deviation}\")\n",
    "\n",
    "\n",
    "# normalize the data -> rescales values so they lie between 0 and 1\n",
    "norm_data = (data - mean)/ standard_deviation\n",
    "print(f\"Normalized data: {norm_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd8c8946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 365 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /home/wanyua/Dev/ML/Python-Machine-Learning-Tutorial/env/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/wanyua/Dev/ML/Python-Machine-Learning-Tutorial/env/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[K     |████████████████████████████████| 347 kB 144 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/wanyua/Dev/ML/Python-Machine-Learning-Tutorial/env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "# pandas -dataframes and series\n",
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1b2bb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "3    4\n",
       "4    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# series - one dimension array like object. Similar like a column in a table\n",
    "data = [1,2,3,4,5]\n",
    "series = pd.Series(data)\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cede446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1\n",
       "b    2\n",
       "c    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a series from a dictionary\n",
    "data = {'a': 1, 'b': 2, 'c': 3}\n",
    "series_dict = pd.Series(data)\n",
    "series_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e142bbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    1.0\n",
       "b    2.0\n",
       "c    3.0\n",
       "d    NaN\n",
       "e    NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [1,2,3,4.3,5.3]\n",
    "ind = ['a', 'b', 'c', 'd', 'e']\n",
    "pd.Series(data, index = ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b15917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name  Age        City\n",
      "0    Sam   23  New Mexico\n",
      "1   Vule   23     Nairobi\n",
      "2  Chels   45    Pretoria\n",
      "3    Ros   43       Cairo\n",
      "[['Sam' 23 'New Mexico']\n",
      " ['Vule' 23 'Nairobi']\n",
      " ['Chels' 45 'Pretoria']\n",
      " ['Ros' 43 'Cairo']]\n"
     ]
    }
   ],
   "source": [
    "# Data frame - similar to a table or a multidimensional array\n",
    "# creating a df from a dictionary\n",
    "data = {\n",
    "    'Name': ['Sam', 'Vule', 'Chels', 'Ros'],\n",
    "    'Age': [23,23,45,43],\n",
    "    'City': [\"New Mexico\", 'Nairobi', \"Pretoria\", \"Cairo\"]\n",
    "}\n",
    "\n",
    "print(pd.DataFrame(data))\n",
    "print(np.array(pd.DataFrame(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "832aeb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name  age        city\n",
      "0  Sam   25  new jersey\n",
      "1  Sam   25  new jersey\n",
      "2  Sam   25  new jersey\n",
      "3  Sam   25  new jersey\n",
      "4  Sam   25  new jersey\n"
     ]
    }
   ],
   "source": [
    "# creating a df from a list of dictionary\n",
    "data = [\n",
    "    {'name': 'Sam', 'age': 25, 'city': 'new jersey'},\n",
    "    {'name': 'Sam', 'age': 25, 'city': 'new jersey'},\n",
    "    {'name': 'Sam', 'age': 25, 'city': 'new jersey'},\n",
    "    {'name': 'Sam', 'age': 25, 'city': 'new jersey'},\n",
    "    {'name': 'Sam', 'age': 25, 'city': 'new jersey'},\n",
    "]\n",
    "\n",
    "print(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea090325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f6d3e3d2760>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /packages/5c/e7/71927b088047132317c14eb513d69c8375ddba3c9029d4154a054f6c8765/kagglehub-0.2.9-py3-none-any.whl\u001b[0m\n",
      "  Downloading kagglehub-0.2.9-py3-none-any.whl (39 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 412 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 304 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/wanyua/Dev/ML/Python-Machine-Learning-Tutorial/env/lib/python3.8/site-packages (from kagglehub) (25.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 262 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 157 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 147 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 202 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm, charset-normalizer, idna, urllib3, certifi, requests, kagglehub\n",
      "Successfully installed certifi-2025.4.26 charset-normalizer-3.4.2 idna-3.10 kagglehub-0.2.9 requests-2.32.3 tqdm-4.67.1 urllib3-2.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanyua/Dev/ML/Python-Machine-Learning-Tutorial/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ivankmk/thousand-ml-jobs-in-usa?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.60M/1.60M [00:02<00:00, 794kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model files...\n",
      "Path to dataset files: /home/wanyua/.cache/kagglehub/datasets/ivankmk/thousand-ml-jobs-in-usa/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ivankmk/thousand-ml-jobs-in-usa\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "538b7d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>company_address_locality</th>\n",
       "      <th>company_address_region</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_website</th>\n",
       "      <th>company_description</th>\n",
       "      <th>job_description_text</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>job_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>Upper Hand</td>\n",
       "      <td>https://upperhand.com</td>\n",
       "      <td>Upper Hand is the leading provider of full-sui...</td>\n",
       "      <td>OverviewUpper Hand is embarking on an exciting...</td>\n",
       "      <td>Internship</td>\n",
       "      <td>Internship - Machine Learning Engineer &amp; Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-03-14</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>Ikigai</td>\n",
       "      <td>https://www.ikigailabs.io</td>\n",
       "      <td>Built upon years of MIT research, Ikigai is a ...</td>\n",
       "      <td>Company DescriptionThe Ikigai platform unlocks...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-04-09</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>Adobe</td>\n",
       "      <td>http://www.adobe.com</td>\n",
       "      <td>Adobe is the global leader in digital media an...</td>\n",
       "      <td>Our CompanyChanging the world through digital ...</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>California</td>\n",
       "      <td>Waymo</td>\n",
       "      <td>https://waymo.com/careers/</td>\n",
       "      <td>On the journey to be the world's most trusted ...</td>\n",
       "      <td>Waymo is an autonomous driving technology comp...</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Machine Learning Engineer, Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-03-28</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>HMH</td>\n",
       "      <td>http://www.hmhco.com</td>\n",
       "      <td>We are an adaptive learning company that empow...</td>\n",
       "      <td>Job Title: Machine Learning EngineerLocation: ...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 job_posted_date company_address_locality company_address_region  \\\n",
       "0           0      2024-10-31             Indianapolis                Indiana   \n",
       "1           1      2025-03-14            San Francisco             California   \n",
       "2           2      2025-04-09                 San Jose                     CA   \n",
       "3           3      2025-03-22            Mountain View             California   \n",
       "4           4      2025-03-28                   Boston          Massachusetts   \n",
       "\n",
       "  company_name             company_website  \\\n",
       "0   Upper Hand       https://upperhand.com   \n",
       "1       Ikigai   https://www.ikigailabs.io   \n",
       "2        Adobe        http://www.adobe.com   \n",
       "3        Waymo  https://waymo.com/careers/   \n",
       "4          HMH        http://www.hmhco.com   \n",
       "\n",
       "                                 company_description  \\\n",
       "0  Upper Hand is the leading provider of full-sui...   \n",
       "1  Built upon years of MIT research, Ikigai is a ...   \n",
       "2  Adobe is the global leader in digital media an...   \n",
       "3  On the journey to be the world's most trusted ...   \n",
       "4  We are an adaptive learning company that empow...   \n",
       "\n",
       "                                job_description_text   seniority_level  \\\n",
       "0  OverviewUpper Hand is embarking on an exciting...        Internship   \n",
       "1  Company DescriptionThe Ikigai platform unlocks...  Mid-Senior level   \n",
       "2  Our CompanyChanging the world through digital ...       Entry level   \n",
       "3  Waymo is an autonomous driving technology comp...       Entry level   \n",
       "4  Job Title: Machine Learning EngineerLocation: ...  Mid-Senior level   \n",
       "\n",
       "                                           job_title  \n",
       "0  Internship - Machine Learning Engineer & Data ...  \n",
       "1                          Machine Learning Engineer  \n",
       "2                          Machine Learning Engineer  \n",
       "3                Machine Learning Engineer, Training  \n",
       "4                          Machine Learning Engineer  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('1000_ml_jobs_us.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cc3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                                  2\n",
       "job_posted_date                                                    2025-04-09\n",
       "company_address_locality                                             San Jose\n",
       "company_address_region                                                     CA\n",
       "company_name                                                            Adobe\n",
       "company_website                                          http://www.adobe.com\n",
       "company_description         Adobe is the global leader in digital media an...\n",
       "job_description_text        Our CompanyChanging the world through digital ...\n",
       "seniority_level                                                   Entry level\n",
       "job_title                                           Machine Learning Engineer\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2] # this is a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9714b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            Indiana\n",
       "1         California\n",
       "2                 CA\n",
       "3         California\n",
       "4      Massachusetts\n",
       "           ...      \n",
       "992               CA\n",
       "993            Texas\n",
       "994               NY\n",
       "995               CA\n",
       "996    Massachusetts\n",
       "Name: company_address_region, Length: 997, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0:,3] # column 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "468d4a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_posted_date', 'company_address_locality', 'company_address_region',\n",
       "       'company_name', 'company_website', 'company_description',\n",
       "       'job_description_text', 'seniority_level', 'job_title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove a column\n",
    "df.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2584ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
